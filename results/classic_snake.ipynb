{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3898a24",
   "metadata": {},
   "source": [
    "## Classic snake DQN\n",
    "Using the DQN(with experienced replay) implementation of Google-deepmind(2013) to train an agent to play the classic snake game\n",
    "- The **DQNagent** class handles the replay memory and the 2 sets of networks. Implements a modified version of the archietecture specified by DeepMind.\n",
    "- The **snake_RL_env** provides a custom classic snake environment, provides an interface for the agent and is specifically designed for RL tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "015f6d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snake_RL_env\n",
    "from DQN_agent import DQNagent\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78ab9a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6d41c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.19.0\n",
      "GPU avaible: []\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensorflow version:\", tf.__version__)\n",
    "print(\"GPU avaible:\", tf.config.list_physical_devices('GPU')  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5186525a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_period=50\n",
    "num_episodes=1000\n",
    "record_performance_every=50\n",
    "min_e=.01\n",
    "total_e_decay_cycles=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70338913",
   "metadata": {},
   "source": [
    "#### **Initialising the environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b8384f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:296: UserWarning: \u001b[33mWARN: Overwriting existing videos at /Users/rohanchandra/Documents/RL_algorithms/DQNs/classic_snake/Classic_snake_gym_vid folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env=gym.make(\"snake_RL_env/ClassicSnake-v0\", render_mode=\"rgb_array\")\n",
    "env = RecordVideo(\n",
    "    env,\n",
    "    video_folder=\"Classic_snake_gym_vid\",\n",
    "    name_prefix=\"training\",\n",
    "    episode_trigger=lambda x: x % training_period == 0  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f2aaa7",
   "metadata": {},
   "source": [
    "#### **Initialising the agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e11e5450",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">800</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,040</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">164</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m16\u001b[0m)       │           \u001b[38;5;34m528\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m4,640\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m800\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m)             │        \u001b[38;5;34m32,040\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │           \u001b[38;5;34m164\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">37,372</span> (145.98 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m37,372\u001b[0m (145.98 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">37,372</span> (145.98 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m37,372\u001b[0m (145.98 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent=DQNagent.agent(env.unwrapped.size, env.unwrapped.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372ba4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.06, Episode_reward: -0.31\n",
      "Time: 0.00, Episode_reward: -0.25\n",
      "Time: 0.00, Episode_reward: -1.11\n",
      "Time: 0.00, Episode_reward: -1.11\n",
      "Time: 0.00, Episode_reward: -0.18\n",
      "Time: 0.00, Episode_reward: -1.21\n",
      "Time: 0.00, Episode_reward: -0.29\n",
      "Time: 0.00, Episode_reward: -0.29\n",
      "Time: 0.00, Episode_reward: -1.28\n",
      "Time: 0.00, Episode_reward: -1.26\n",
      "Time: 0.00, Episode_reward: -0.29\n",
      "Time: 0.00, Episode_reward: -0.29\n",
      "Time: 0.00, Episode_reward: -1.07\n",
      "Time: 0.00, Episode_reward: -0.29\n",
      "Time: 0.00, Episode_reward: -1.22\n",
      "Time: 0.00, Episode_reward: -1.05\n",
      "Time: 0.00, Episode_reward: -1.15\n",
      "Time: 0.00, Episode_reward: -0.29\n",
      "Time: 0.00, Episode_reward: -0.05\n",
      "Time: 0.00, Episode_reward: -1.17\n",
      "Time: 0.00, Episode_reward: -1.12\n",
      "Time: 0.00, Episode_reward: -1.17\n",
      "Time: 0.00, Episode_reward: -1.09\n",
      "Time: 0.00, Episode_reward: -0.29\n",
      "Time: 0.00, Episode_reward: -1.23\n",
      "Time: 0.00, Episode_reward: -0.29\n",
      "Time: 0.00, Episode_reward: -0.29\n",
      "Time: 0.00, Episode_reward: -0.29\n",
      "Time: 0.00, Episode_reward: -0.29\n",
      "Time: 0.00, Episode_reward: -0.29\n",
      "Time: 0.00, Episode_reward: -0.15\n",
      "Time: 0.00, Episode_reward: -0.23\n",
      "Time: 0.00, Episode_reward: -1.25\n",
      "Time: 0.00, Episode_reward: -0.29\n",
      "Time: 0.00, Episode_reward: -0.29\n",
      "Time: 0.00, Episode_reward: -1.23\n",
      "Time: 0.00, Episode_reward: -0.22\n",
      "Time: 0.00, Episode_reward: -1.10\n",
      "Time: 0.00, Episode_reward: -0.29\n",
      "Time: 0.00, Episode_reward: -1.16\n",
      "Time: 0.00, Episode_reward: -1.11\n",
      "Time: 0.00, Episode_reward: -1.19\n",
      "Time: 0.00, Episode_reward: -1.15\n",
      "Time: 0.00, Episode_reward: -0.29\n",
      "Time: 20.88, Episode_reward: -0.29\n",
      "Time: 54.05, Episode_reward: -0.29\n",
      "Time: 54.09, Episode_reward: -0.29\n",
      "Time: 25.95, Episode_reward: -1.13\n",
      "Time: 54.13, Episode_reward: -0.29\n",
      "Time: 40.95, Episode_reward: -0.20\n",
      "Time: 43.49, Episode_reward: -1.22\n",
      "Time: 53.98, Episode_reward: -0.29\n",
      "Time: 41.20, Episode_reward: -1.21\n",
      "Time: 44.07, Episode_reward: -1.23\n",
      "Time: 53.67, Episode_reward: -1.28\n",
      "Time: 53.56, Episode_reward: -0.29\n",
      "Time: 21.52, Episode_reward: -1.11\n",
      "Time: 17.23, Episode_reward: -1.08\n",
      "Time: 25.09, Episode_reward: -1.12\n",
      "Time: 37.22, Episode_reward: -1.19\n",
      "Time: 47.60, Episode_reward: -1.25\n",
      "Time: 53.60, Episode_reward: -0.29\n",
      "Time: 54.23, Episode_reward: -0.29\n",
      "Time: 15.98, Episode_reward: -1.08\n",
      "Time: 53.14, Episode_reward: -1.28\n",
      "Time: 53.17, Episode_reward: -0.29\n",
      "Time: 53.68, Episode_reward: 0.72\n",
      "Time: 44.59, Episode_reward: -1.23\n",
      "Time: 25.64, Episode_reward: -0.12\n",
      "Time: 80.42, Episode_reward: 0.59\n",
      "Time: 27.80, Episode_reward: -1.14\n",
      "Time: 32.92, Episode_reward: -1.17\n",
      "Time: 53.51, Episode_reward: -0.29\n",
      "Time: 23.45, Episode_reward: -1.12\n",
      "Time: 52.83, Episode_reward: -0.29\n",
      "Time: 52.87, Episode_reward: -0.29\n",
      "Time: 51.06, Episode_reward: -1.27\n",
      "Time: 52.71, Episode_reward: -0.29\n",
      "Time: 52.82, Episode_reward: -0.29\n",
      "Time: 53.81, Episode_reward: -0.29\n",
      "Time: 15.06, Episode_reward: -1.07\n",
      "Time: 32.71, Episode_reward: -1.17\n",
      "Time: 25.80, Episode_reward: -1.13\n",
      "Time: 34.97, Episode_reward: -1.18\n",
      "Time: 52.99, Episode_reward: -0.29\n",
      "Time: 47.37, Episode_reward: -1.25\n",
      "Time: 52.90, Episode_reward: -0.29\n",
      "Time: 53.24, Episode_reward: -0.29\n",
      "Time: 52.99, Episode_reward: -0.29\n",
      "Time: 45.41, Episode_reward: -0.23\n",
      "Time: 14.35, Episode_reward: -1.07\n",
      "Time: 29.25, Episode_reward: -1.15\n",
      "Time: 53.20, Episode_reward: -0.29\n",
      "Time: 34.46, Episode_reward: -1.18\n",
      "Time: 52.90, Episode_reward: -0.29\n",
      "Time: 25.92, Episode_reward: -0.12\n",
      "Time: 24.06, Episode_reward: -1.12\n",
      "Time: 53.38, Episode_reward: -0.29\n",
      "Time: 53.54, Episode_reward: -0.29\n",
      "Time: 20.19, Episode_reward: -1.10\n",
      "Time: 21.87, Episode_reward: -1.11\n",
      "Time: 27.07, Episode_reward: -1.14\n",
      "Time: 27.64, Episode_reward: -1.14\n",
      "Time: 53.24, Episode_reward: -0.29\n",
      "Time: 29.40, Episode_reward: -1.15\n",
      "Time: 18.42, Episode_reward: -1.09\n",
      "Time: 23.67, Episode_reward: -1.12\n",
      "Time: 42.25, Episode_reward: -0.21\n",
      "Time: 35.20, Episode_reward: -0.17\n",
      "Time: 48.22, Episode_reward: -1.25\n",
      "Time: 45.80, Episode_reward: -1.24\n",
      "Time: 52.92, Episode_reward: -0.29\n",
      "Time: 53.19, Episode_reward: -0.29\n",
      "Time: 53.77, Episode_reward: 0.72\n",
      "Time: 62.92, Episode_reward: -0.32\n",
      "Time: 88.66, Episode_reward: 0.54\n",
      "Time: 53.51, Episode_reward: -0.29\n",
      "Time: 42.44, Episode_reward: -0.21\n",
      "Time: 31.62, Episode_reward: -1.16\n",
      "Time: 53.39, Episode_reward: -0.29\n",
      "Time: 53.59, Episode_reward: -0.29\n",
      "Time: 12.82, Episode_reward: -1.06\n",
      "Time: 47.97, Episode_reward: -1.25\n",
      "Time: 38.58, Episode_reward: -1.20\n",
      "Time: 29.16, Episode_reward: -1.15\n",
      "Time: 53.71, Episode_reward: -0.29\n",
      "Time: 53.54, Episode_reward: -0.29\n",
      "Time: 53.74, Episode_reward: -0.29\n",
      "Time: 86.54, Episode_reward: -0.45\n",
      "Time: 53.50, Episode_reward: -0.29\n",
      "Time: 53.95, Episode_reward: -0.29\n",
      "Time: 24.49, Episode_reward: -1.12\n",
      "Time: 14.32, Episode_reward: -1.07\n",
      "Time: 33.66, Episode_reward: -1.17\n",
      "Time: 53.39, Episode_reward: -0.29\n",
      "Time: 53.51, Episode_reward: -0.29\n",
      "Time: 53.42, Episode_reward: -0.29\n",
      "Time: 53.12, Episode_reward: -0.29\n",
      "Time: 53.74, Episode_reward: -0.29\n",
      "Time: 20.55, Episode_reward: -0.09\n",
      "Time: 42.15, Episode_reward: -1.22\n",
      "Time: 53.68, Episode_reward: -0.29\n",
      "Time: 35.09, Episode_reward: -1.18\n",
      "Time: 53.60, Episode_reward: -0.29\n",
      "Time: 13.11, Episode_reward: -1.06\n",
      "Time: 53.76, Episode_reward: -0.29\n",
      "Time: 25.65, Episode_reward: -1.13\n",
      "Time: 42.78, Episode_reward: -1.22\n",
      "Time: 29.35, Episode_reward: -1.15\n",
      "Time: 25.83, Episode_reward: -1.13\n",
      "Time: 53.49, Episode_reward: -0.29\n",
      "Time: 53.09, Episode_reward: -0.29\n",
      "Time: 33.33, Episode_reward: -0.16\n",
      "Time: 23.73, Episode_reward: -1.12\n",
      "Time: 29.83, Episode_reward: -1.15\n",
      "Time: 53.82, Episode_reward: -0.29\n",
      "Time: 53.68, Episode_reward: -0.29\n",
      "Time: 71.48, Episode_reward: 0.62\n",
      "Time: 42.55, Episode_reward: -1.22\n",
      "Time: 53.08, Episode_reward: -0.29\n",
      "Time: 53.78, Episode_reward: -0.29\n",
      "Time: 32.26, Episode_reward: -1.16\n",
      "Time: 32.72, Episode_reward: -1.17\n",
      "Time: 53.51, Episode_reward: -0.29\n",
      "Time: 54.08, Episode_reward: -0.29\n",
      "Time: 51.70, Episode_reward: -0.29\n",
      "Time: 51.55, Episode_reward: -0.29\n",
      "Time: 44.09, Episode_reward: -1.24\n",
      "Time: 51.74, Episode_reward: -0.29\n",
      "Time: 77.56, Episode_reward: -0.42\n",
      "Time: 47.85, Episode_reward: -1.26\n",
      "Time: 51.82, Episode_reward: -0.29\n",
      "Time: 28.52, Episode_reward: -1.15\n",
      "Time: 40.92, Episode_reward: -1.22\n",
      "Time: 51.28, Episode_reward: -0.29\n",
      "Time: 37.73, Episode_reward: -1.20\n",
      "Time: 51.42, Episode_reward: -0.29\n",
      "Time: 31.57, Episode_reward: -1.17\n",
      "Time: 16.52, Episode_reward: -1.08\n",
      "Time: 27.63, Episode_reward: -1.15\n",
      "Time: 51.20, Episode_reward: -0.29\n",
      "Time: 13.02, Episode_reward: -1.06\n",
      "Time: 29.37, Episode_reward: -1.16\n",
      "Time: 33.88, Episode_reward: -1.18\n",
      "Time: 51.92, Episode_reward: -0.29\n",
      "Time: 21.54, Episode_reward: -1.11\n",
      "Time: 18.30, Episode_reward: -1.09\n",
      "Time: 52.25, Episode_reward: -0.29\n",
      "Time: 51.68, Episode_reward: -0.29\n",
      "Time: 51.62, Episode_reward: -0.29\n",
      "Time: 51.31, Episode_reward: -0.29\n",
      "Time: 34.37, Episode_reward: -1.18\n",
      "Time: 20.90, Episode_reward: -1.11\n",
      "Time: 81.64, Episode_reward: -0.44\n",
      "Time: 44.74, Episode_reward: -0.23\n",
      "Time: 21.26, Episode_reward: -1.11\n",
      "Time: 14.87, Episode_reward: -1.07\n",
      "Time: 51.58, Episode_reward: -0.29\n",
      "Time: 51.62, Episode_reward: -0.29\n",
      "Time: 11.94, Episode_reward: -1.06\n",
      "Time: 12.32, Episode_reward: -1.06\n",
      "Time: 18.74, Episode_reward: -1.10\n",
      "Time: 40.87, Episode_reward: -0.21\n",
      "Time: 51.41, Episode_reward: -0.29\n",
      "Time: 21.66, Episode_reward: -1.11\n",
      "Time: 26.26, Episode_reward: -0.13\n",
      "Time: 51.35, Episode_reward: -0.29\n",
      "Time: 51.13, Episode_reward: -0.29\n",
      "Time: 30.66, Episode_reward: -0.15\n",
      "Time: 51.68, Episode_reward: -0.29\n",
      "Time: 51.30, Episode_reward: -0.29\n",
      "Time: 33.97, Episode_reward: -1.18\n",
      "Time: 22.37, Episode_reward: -0.11\n",
      "Time: 23.50, Episode_reward: -1.12\n",
      "Time: 51.55, Episode_reward: -0.29\n",
      "Time: 51.51, Episode_reward: -1.28\n",
      "Time: 51.27, Episode_reward: -0.29\n",
      "Time: 29.64, Episode_reward: -1.16\n",
      "Time: 11.11, Episode_reward: -0.04\n",
      "Time: 51.66, Episode_reward: -0.29\n",
      "Time: 13.43, Episode_reward: -1.07\n",
      "Time: 25.16, Episode_reward: -1.13\n",
      "Time: 33.35, Episode_reward: -1.18\n",
      "Time: 51.41, Episode_reward: -0.29\n",
      "Time: 28.45, Episode_reward: -1.15\n",
      "Time: 51.34, Episode_reward: -0.29\n",
      "Time: 46.13, Episode_reward: -1.25\n",
      "Time: 30.31, Episode_reward: -0.15\n",
      "Time: 60.01, Episode_reward: 1.70\n",
      "Time: 51.01, Episode_reward: -0.29\n",
      "Time: 39.20, Episode_reward: -1.21\n",
      "Time: 34.80, Episode_reward: -1.19\n",
      "Time: 21.35, Episode_reward: -1.11\n",
      "Time: 30.02, Episode_reward: -1.16\n",
      "Time: 27.92, Episode_reward: -1.15\n",
      "Time: 21.42, Episode_reward: -1.11\n",
      "Time: 51.42, Episode_reward: -0.29\n",
      "Time: 34.94, Episode_reward: -1.19\n",
      "Time: 21.59, Episode_reward: -1.11\n",
      "Time: 50.85, Episode_reward: -0.29\n",
      "Time: 51.10, Episode_reward: -0.29\n",
      "Time: 24.69, Episode_reward: -1.13\n",
      "Time: 29.81, Episode_reward: -0.15\n",
      "Time: 21.10, Episode_reward: -1.11\n",
      "Time: 43.99, Episode_reward: -0.23\n",
      "Time: 14.46, Episode_reward: -1.07\n",
      "Time: 51.00, Episode_reward: -0.29\n",
      "Time: 36.97, Episode_reward: -1.20\n",
      "Time: 51.36, Episode_reward: -0.29\n",
      "Time: 51.46, Episode_reward: -0.29\n",
      "Time: 51.07, Episode_reward: -0.29\n",
      "Time: 18.49, Episode_reward: -1.10\n",
      "Time: 51.39, Episode_reward: -0.29\n",
      "Time: 9.05, Episode_reward: -1.04\n",
      "Time: 56.88, Episode_reward: -0.30\n",
      "Time: 12.51, Episode_reward: -0.05\n",
      "Time: 40.78, Episode_reward: -1.22\n",
      "Time: 20.76, Episode_reward: -1.11\n",
      "Time: 51.17, Episode_reward: 0.74\n",
      "Time: 51.32, Episode_reward: -0.29\n",
      "Time: 66.17, Episode_reward: -0.35\n",
      "Time: 42.12, Episode_reward: -1.23\n",
      "Time: 17.68, Episode_reward: -1.09\n",
      "Time: 50.86, Episode_reward: -0.29\n",
      "Time: 51.05, Episode_reward: -0.29\n",
      "Time: 42.62, Episode_reward: -1.23\n",
      "Time: 44.17, Episode_reward: -1.24\n",
      "Time: 17.11, Episode_reward: -1.09\n",
      "Time: 51.31, Episode_reward: -0.29\n",
      "Time: 42.30, Episode_reward: -1.23\n",
      "Time: 51.28, Episode_reward: -0.29\n",
      "Time: 28.76, Episode_reward: -1.15\n",
      "Time: 33.29, Episode_reward: -0.17\n",
      "Time: 17.84, Episode_reward: -1.09\n",
      "Time: 51.36, Episode_reward: -0.29\n",
      "Time: 37.40, Episode_reward: -1.20\n",
      "Time: 51.50, Episode_reward: -0.29\n",
      "Time: 45.99, Episode_reward: 0.77\n",
      "Time: 22.03, Episode_reward: -1.12\n",
      "Time: 21.90, Episode_reward: 0.91\n",
      "Time: 36.60, Episode_reward: -1.20\n",
      "Time: 51.72, Episode_reward: -0.29\n",
      "Time: 51.59, Episode_reward: -0.29\n",
      "Time: 51.52, Episode_reward: -0.29\n",
      "Time: 17.40, Episode_reward: -1.09\n",
      "Time: 41.13, Episode_reward: -1.22\n",
      "Time: 51.66, Episode_reward: -0.29\n",
      "Time: 60.05, Episode_reward: -0.32\n",
      "Time: 37.03, Episode_reward: -0.19\n",
      "Time: 51.74, Episode_reward: -0.29\n",
      "Time: 25.35, Episode_reward: -1.13\n",
      "Time: 52.38, Episode_reward: -0.29\n",
      "Time: 51.65, Episode_reward: -0.29\n",
      "Time: 44.45, Episode_reward: -1.24\n",
      "Time: 51.99, Episode_reward: 0.72\n",
      "Time: 24.37, Episode_reward: -1.13\n",
      "Time: 51.95, Episode_reward: -0.29\n",
      "Time: 46.01, Episode_reward: -1.25\n",
      "Time: 51.69, Episode_reward: -0.29\n",
      "Time: 51.42, Episode_reward: -0.29\n",
      "Time: 25.17, Episode_reward: -1.13\n",
      "Time: 38.40, Episode_reward: -1.21\n",
      "Time: 31.48, Episode_reward: -1.17\n",
      "Time: 52.06, Episode_reward: -0.29\n",
      "Time: 51.28, Episode_reward: -0.29\n",
      "Time: 32.16, Episode_reward: -1.17\n",
      "Time: 49.66, Episode_reward: -1.27\n",
      "Time: 37.71, Episode_reward: -1.20\n",
      "Time: 24.23, Episode_reward: -1.13\n",
      "Time: 51.57, Episode_reward: -0.29\n",
      "Time: 37.22, Episode_reward: -1.20\n",
      "Time: 25.47, Episode_reward: -0.12\n",
      "Time: 42.56, Episode_reward: -0.22\n",
      "Time: 51.56, Episode_reward: -1.28\n",
      "Time: 51.27, Episode_reward: -0.29\n",
      "Time: 51.33, Episode_reward: -0.29\n",
      "Time: 51.08, Episode_reward: -0.29\n",
      "Time: 31.60, Episode_reward: -0.16\n",
      "Time: 33.43, Episode_reward: -0.17\n",
      "Time: 51.46, Episode_reward: -0.29\n",
      "Time: 37.16, Episode_reward: -1.20\n",
      "Time: 51.06, Episode_reward: -0.29\n",
      "Time: 46.01, Episode_reward: -1.25\n",
      "Time: 32.17, Episode_reward: -1.17\n",
      "Time: 51.53, Episode_reward: -0.29\n",
      "Time: 29.85, Episode_reward: -1.16\n",
      "Time: 51.39, Episode_reward: -0.29\n",
      "Time: 51.85, Episode_reward: -0.29\n",
      "Time: 51.59, Episode_reward: -0.29\n",
      "Time: 44.65, Episode_reward: -0.23\n",
      "Time: 28.35, Episode_reward: -1.15\n",
      "Time: 94.64, Episode_reward: 0.48\n",
      "Time: 51.96, Episode_reward: -0.29\n",
      "Time: 71.82, Episode_reward: -0.38\n",
      "Time: 41.11, Episode_reward: -0.21\n",
      "Time: 10.72, Episode_reward: -1.05\n",
      "Time: 52.04, Episode_reward: -0.29\n",
      "Time: 39.30, Episode_reward: -1.21\n",
      "Time: 52.00, Episode_reward: -0.29\n",
      "Time: 24.26, Episode_reward: -1.13\n",
      "Time: 51.70, Episode_reward: -0.29\n",
      "Time: 51.22, Episode_reward: -0.29\n",
      "Time: 51.76, Episode_reward: -0.29\n",
      "Time: 51.65, Episode_reward: -0.29\n",
      "Time: 51.52, Episode_reward: -0.29\n",
      "Time: 33.59, Episode_reward: -1.18\n",
      "Time: 23.68, Episode_reward: -1.12\n",
      "Time: 51.73, Episode_reward: -0.29\n",
      "Time: 50.99, Episode_reward: -0.29\n",
      "Time: 15.42, Episode_reward: -1.08\n",
      "Time: 51.41, Episode_reward: -0.29\n",
      "Time: 51.45, Episode_reward: -0.29\n",
      "Time: 61.74, Episode_reward: -0.33\n",
      "Time: 38.02, Episode_reward: -1.20\n",
      "Time: 51.63, Episode_reward: -0.29\n",
      "Time: 50.19, Episode_reward: -1.27\n",
      "Time: 51.68, Episode_reward: -0.29\n",
      "Time: 18.01, Episode_reward: -1.09\n",
      "Time: 51.66, Episode_reward: 0.72\n",
      "Time: 48.02, Episode_reward: -1.26\n",
      "Time: 66.11, Episode_reward: -0.35\n",
      "Time: 34.07, Episode_reward: -1.18\n",
      "Time: 54.53, Episode_reward: -0.29\n",
      "Time: 54.80, Episode_reward: 0.72\n",
      "Time: 21.73, Episode_reward: -1.11\n",
      "Time: 53.81, Episode_reward: -0.29\n",
      "Time: 30.46, Episode_reward: -1.15\n",
      "Time: 33.00, Episode_reward: -1.17\n",
      "Time: 85.45, Episode_reward: 0.57\n",
      "Time: 53.79, Episode_reward: -0.29\n",
      "Time: 30.33, Episode_reward: -1.15\n",
      "Time: 44.24, Episode_reward: -0.22\n",
      "Time: 29.37, Episode_reward: -1.15\n",
      "Time: 18.85, Episode_reward: -1.09\n",
      "Time: 85.43, Episode_reward: 1.56\n",
      "Time: 54.13, Episode_reward: -0.29\n",
      "Time: 54.42, Episode_reward: -0.29\n",
      "Time: 33.35, Episode_reward: -0.16\n",
      "Time: 30.48, Episode_reward: -0.14\n",
      "Time: 40.16, Episode_reward: -1.21\n",
      "Time: 23.85, Episode_reward: -0.11\n",
      "Time: 33.94, Episode_reward: -1.17\n",
      "Time: 19.94, Episode_reward: -1.10\n",
      "Time: 53.66, Episode_reward: -0.29\n",
      "Time: 53.85, Episode_reward: -0.29\n",
      "Time: 53.98, Episode_reward: -0.29\n",
      "Time: 53.93, Episode_reward: 0.72\n",
      "Time: 33.35, Episode_reward: -1.17\n",
      "Time: 33.76, Episode_reward: -1.17\n",
      "Time: 26.89, Episode_reward: -1.14\n",
      "Time: 35.09, Episode_reward: -1.18\n",
      "Time: 53.55, Episode_reward: -0.29\n",
      "Time: 54.20, Episode_reward: -0.29\n",
      "Time: 40.92, Episode_reward: -1.20\n",
      "Time: 57.55, Episode_reward: -0.29\n",
      "Time: 56.94, Episode_reward: -1.28\n",
      "Time: 56.45, Episode_reward: -0.29\n",
      "Time: 39.47, Episode_reward: -1.19\n",
      "Time: 56.60, Episode_reward: -0.29\n",
      "Time: 47.58, Episode_reward: -1.23\n",
      "Time: 70.08, Episode_reward: -0.34\n",
      "Time: 19.14, Episode_reward: -0.08\n",
      "Time: 65.96, Episode_reward: 0.68\n",
      "Time: 59.35, Episode_reward: 0.72\n",
      "Time: 58.21, Episode_reward: -0.29\n",
      "Time: 12.30, Episode_reward: -1.05\n",
      "Time: 13.67, Episode_reward: -0.05\n",
      "Time: 44.87, Episode_reward: -0.20\n",
      "Time: 27.45, Episode_reward: -1.13\n",
      "Time: 62.12, Episode_reward: -0.29\n",
      "Time: 62.95, Episode_reward: -0.29\n",
      "Time: 32.21, Episode_reward: -1.14\n",
      "Time: 33.86, Episode_reward: -0.16\n",
      "Time: 56.53, Episode_reward: -0.29\n",
      "Time: 43.92, Episode_reward: -1.23\n",
      "Time: 55.58, Episode_reward: -0.29\n",
      "Time: 54.43, Episode_reward: -0.29\n",
      "Time: 21.59, Episode_reward: -1.11\n",
      "Time: 34.51, Episode_reward: -1.17\n",
      "Time: 44.20, Episode_reward: -1.22\n",
      "Time: 52.42, Episode_reward: -0.29\n",
      "Time: 22.59, Episode_reward: -1.12\n",
      "Time: 52.34, Episode_reward: -0.29\n",
      "Time: 52.73, Episode_reward: -0.29\n",
      "Time: 53.24, Episode_reward: -0.29\n",
      "Time: 20.36, Episode_reward: -1.10\n",
      "Time: 17.63, Episode_reward: -0.08\n",
      "Time: 50.71, Episode_reward: -1.27\n",
      "Time: 47.90, Episode_reward: -1.25\n",
      "Time: 44.37, Episode_reward: -1.23\n",
      "Time: 53.37, Episode_reward: -0.29\n",
      "Time: 53.76, Episode_reward: -0.29\n",
      "Time: 24.66, Episode_reward: -1.12\n",
      "Time: 38.33, Episode_reward: -0.19\n",
      "Time: 52.97, Episode_reward: -0.29\n",
      "Time: 86.40, Episode_reward: -0.46\n",
      "Time: 53.02, Episode_reward: -0.29\n",
      "Time: 58.60, Episode_reward: -0.30\n",
      "Time: 36.22, Episode_reward: -1.19\n",
      "Time: 53.12, Episode_reward: -0.29\n",
      "Time: 22.41, Episode_reward: -0.10\n",
      "Time: 70.16, Episode_reward: 0.62\n",
      "Time: 98.10, Episode_reward: 1.49\n",
      "Time: 50.25, Episode_reward: -0.25\n",
      "Time: 57.55, Episode_reward: -0.29\n",
      "Time: 58.74, Episode_reward: -0.29\n",
      "Time: 41.08, Episode_reward: -0.20\n",
      "Time: 44.61, Episode_reward: -1.22\n",
      "Time: 58.22, Episode_reward: -0.29\n",
      "Time: 52.85, Episode_reward: -1.25\n",
      "Time: 32.85, Episode_reward: -1.15\n",
      "Time: 27.84, Episode_reward: -1.13\n",
      "Time: 86.91, Episode_reward: 0.61\n",
      "Time: 14.05, Episode_reward: -1.06\n",
      "Time: 60.59, Episode_reward: -0.29\n",
      "Time: 49.95, Episode_reward: -0.22\n",
      "Time: 60.05, Episode_reward: -0.29\n",
      "Time: 56.35, Episode_reward: -0.29\n",
      "Time: 14.35, Episode_reward: -1.06\n",
      "Time: 59.67, Episode_reward: -0.29\n",
      "Time: 59.17, Episode_reward: -0.29\n",
      "Time: 22.00, Episode_reward: -1.10\n",
      "Time: 17.68, Episode_reward: -1.08\n",
      "Time: 45.26, Episode_reward: -0.20\n",
      "Time: 37.51, Episode_reward: -1.17\n",
      "Time: 57.06, Episode_reward: -1.27\n",
      "Time: 54.42, Episode_reward: -1.26\n",
      "Time: 60.26, Episode_reward: -1.28\n",
      "Time: 15.31, Episode_reward: -1.06\n",
      "Time: 59.97, Episode_reward: -0.29\n",
      "Time: 38.90, Episode_reward: -0.15\n",
      "Time: 28.41, Episode_reward: -0.11\n",
      "Time: 29.89, Episode_reward: -1.13\n",
      "Time: 31.68, Episode_reward: -1.14\n",
      "Time: 63.05, Episode_reward: -0.29\n",
      "Time: 50.76, Episode_reward: -1.22\n",
      "Time: 20.69, Episode_reward: -1.08\n",
      "Time: 27.35, Episode_reward: -1.12\n",
      "Time: 39.87, Episode_reward: -1.17\n",
      "Time: 47.90, Episode_reward: -1.21\n",
      "Time: 62.09, Episode_reward: -0.29\n",
      "Time: 45.13, Episode_reward: -1.20\n",
      "Time: 60.96, Episode_reward: -0.29\n",
      "Time: 63.13, Episode_reward: -0.29\n",
      "Time: 53.47, Episode_reward: -1.24\n",
      "Time: 27.20, Episode_reward: -1.11\n",
      "Time: 69.37, Episode_reward: -0.29\n",
      "Time: 64.67, Episode_reward: -0.29\n",
      "Time: 60.84, Episode_reward: -0.29\n",
      "Time: 80.61, Episode_reward: 0.64\n",
      "Time: 64.22, Episode_reward: -0.29\n",
      "Time: 44.29, Episode_reward: -1.20\n",
      "Time: 62.70, Episode_reward: -0.29\n",
      "Time: 49.44, Episode_reward: -1.25\n",
      "Time: 38.34, Episode_reward: -1.20\n",
      "Time: 54.47, Episode_reward: -0.29\n",
      "Time: 50.67, Episode_reward: -1.26\n",
      "Time: 23.43, Episode_reward: -1.11\n",
      "Time: 55.09, Episode_reward: -0.29\n",
      "Time: 40.16, Episode_reward: -1.20\n",
      "Time: 36.14, Episode_reward: -1.19\n",
      "Time: 25.93, Episode_reward: -1.14\n",
      "Time: 38.96, Episode_reward: -0.20\n",
      "Time: 25.71, Episode_reward: -1.13\n",
      "Time: 51.84, Episode_reward: -0.29\n",
      "Time: 52.06, Episode_reward: -0.29\n",
      "Time: 51.62, Episode_reward: 0.72\n",
      "Time: 58.29, Episode_reward: -0.31\n",
      "Time: 43.19, Episode_reward: -1.23\n",
      "Time: 90.90, Episode_reward: 0.50\n",
      "Time: 26.14, Episode_reward: -0.13\n",
      "Time: 51.82, Episode_reward: -0.27\n",
      "Time: 21.65, Episode_reward: -1.11\n",
      "Time: 69.04, Episode_reward: -0.37\n",
      "Time: 51.84, Episode_reward: -0.29\n",
      "Time: 23.46, Episode_reward: -1.12\n",
      "Time: 51.40, Episode_reward: -0.29\n",
      "Time: 51.35, Episode_reward: -0.29\n",
      "Time: 52.25, Episode_reward: -0.29\n",
      "Time: 23.42, Episode_reward: -0.11\n",
      "Time: 51.63, Episode_reward: -0.27\n",
      "Time: 51.21, Episode_reward: -0.29\n",
      "Time: 38.73, Episode_reward: -1.21\n",
      "Time: 51.63, Episode_reward: -0.29\n",
      "Time: 52.02, Episode_reward: -0.29\n",
      "Time: 51.82, Episode_reward: -0.29\n",
      "Time: 50.43, Episode_reward: -1.27\n",
      "Time: 37.26, Episode_reward: -1.20\n",
      "Time: 51.81, Episode_reward: -0.29\n",
      "Time: 51.80, Episode_reward: -0.29\n",
      "Time: 40.94, Episode_reward: -1.22\n",
      "Time: 51.91, Episode_reward: -0.29\n",
      "Time: 51.54, Episode_reward: -0.29\n",
      "Time: 58.79, Episode_reward: -0.31\n",
      "Time: 48.28, Episode_reward: 0.76\n",
      "Time: 51.53, Episode_reward: -0.29\n",
      "Time: 51.54, Episode_reward: -0.29\n",
      "Time: 56.86, Episode_reward: 0.71\n",
      "Time: 30.71, Episode_reward: -1.16\n",
      "Time: 68.47, Episode_reward: 0.64\n",
      "Time: 12.73, Episode_reward: -1.06\n",
      "Time: 87.80, Episode_reward: 0.53\n",
      "Time: 30.05, Episode_reward: -1.16\n",
      "Time: 40.78, Episode_reward: -1.22\n",
      "Time: 51.14, Episode_reward: -0.29\n",
      "Time: 33.65, Episode_reward: -1.18\n",
      "Time: 15.79, Episode_reward: -1.08\n",
      "Time: 51.41, Episode_reward: -0.29\n",
      "Time: 23.14, Episode_reward: -1.12\n",
      "Time: 42.57, Episode_reward: -1.23\n",
      "Time: 51.16, Episode_reward: -0.29\n",
      "Time: 50.95, Episode_reward: 0.72\n",
      "Time: 19.05, Episode_reward: -1.10\n",
      "Time: 51.10, Episode_reward: -1.28\n",
      "Time: 51.75, Episode_reward: -0.29\n",
      "Time: 21.71, Episode_reward: -1.11\n",
      "Time: 25.94, Episode_reward: -1.14\n",
      "Time: 51.69, Episode_reward: -0.29\n",
      "Time: 51.83, Episode_reward: -0.29\n",
      "Time: 19.69, Episode_reward: -0.09\n",
      "Time: 35.86, Episode_reward: -1.19\n",
      "Time: 51.25, Episode_reward: -0.29\n",
      "Time: 51.76, Episode_reward: -0.29\n",
      "Time: 51.42, Episode_reward: -0.29\n",
      "Time: 51.70, Episode_reward: -0.29\n",
      "Time: 47.98, Episode_reward: -1.26\n",
      "Time: 51.67, Episode_reward: -0.29\n",
      "Time: 27.15, Episode_reward: -1.14\n",
      "Time: 52.25, Episode_reward: -0.29\n",
      "Time: 47.97, Episode_reward: -1.26\n",
      "Time: 15.54, Episode_reward: -1.07\n",
      "Time: 14.57, Episode_reward: -0.06\n",
      "Time: 55.08, Episode_reward: -0.29\n",
      "Time: 30.24, Episode_reward: -1.15\n",
      "Time: 25.56, Episode_reward: -1.13\n",
      "Time: 21.52, Episode_reward: -0.09\n",
      "Time: 12.05, Episode_reward: -1.06\n",
      "Time: 54.50, Episode_reward: -0.29\n",
      "Time: 33.42, Episode_reward: -1.17\n",
      "Time: 51.57, Episode_reward: -0.29\n",
      "Time: 45.05, Episode_reward: -1.24\n",
      "Time: 51.90, Episode_reward: -0.29\n",
      "Time: 47.06, Episode_reward: -1.25\n",
      "Time: 28.00, Episode_reward: -1.15\n",
      "Time: 52.52, Episode_reward: -0.29\n",
      "Time: 20.11, Episode_reward: -0.09\n",
      "Time: 28.52, Episode_reward: -1.15\n"
     ]
    }
   ],
   "source": [
    "# they store the avg_rew, min_rew and max_rew arrays after every e_decay_cycle\n",
    "AVG_REW=[]\n",
    "MIN_REW=[]\n",
    "MAX_REW=[]\n",
    "\n",
    "a=agent.env_size\n",
    "b=agent.stack_frame\n",
    "# these note the reward per episode and avg_rew, min_rew, max_rew every certain number of episodes\n",
    "rewards=np.array([0], dtype=np.float64)\n",
    "avg_rew=np.array([0], dtype=np.float64)\n",
    "min_rew=np.array([0], dtype=np.float64)\n",
    "max_rew=np.array([0], dtype=np.float64)\n",
    "\n",
    "e_decay_cycle=1\n",
    "for episode in range(num_episodes+1):\n",
    "    obs1, info= env.reset()\n",
    "    obs2, reward, terminate, truncate, info=env.step(0)\n",
    "    \n",
    "    st_=np.array([obs2,obs1])  #(st+1, st)\n",
    "    st=st_.reshape((a,a,b))\n",
    "    episode_reward=0    #cumulative reward obtained in a given episode\n",
    "    done=0\n",
    "    e=1  \n",
    "    \n",
    "    start_time=time.time()\n",
    "    while not done:     #executing a single episode\n",
    "        action=agent.e_greedy(st, e)\n",
    "        obs, rt, terminate, truncate, info=env.step(action)\n",
    "        st1_=np.array([obs, st_[0]])\n",
    "        st1=st1_.reshape((a,a,b))\n",
    "        agent.update_replay_memory(st, action, rt, st1, terminate)\n",
    "        done = terminate or truncate\n",
    "        agent.train(done)\n",
    "        \n",
    "        st=st1\n",
    "        st_=st1_\n",
    "        episode_reward+=rt\n",
    "        \n",
    "    end_time=time.time()\n",
    "    \n",
    "    print(f\"Time: {end_time - start_time:.2f}, Episode_reward: {episode_reward:.2f}\")\n",
    "\n",
    "    \n",
    "    e=max(min_e, .999*e)    \n",
    "    np.append(rewards, episode_reward)\n",
    "    \n",
    "    if (episode%record_performance_every==0):\n",
    "        Z=rewards[-record_performance_every:]\n",
    "        np.append(avg_rew, np.sum(Z)/Z.shape[0])\n",
    "        np.append(min_rew, np.min(Z))\n",
    "        np.append(max_rew, np.max(Z))\n",
    "        \n",
    "        agent.model.save(f\"checkpoints/dqn_episode_{episode}.keras\")\n",
    "        # PENDING: code to log data in tensorboard using tf.summary\n",
    "    \n",
    "    # this helps our agent escape local minimas, if it ever gets stuck in one\n",
    "    '''if(episode == e_decay_cycle*num_episodes // total_e_decay_cycles):\n",
    "        e=1\n",
    "        e_decay_cycle+=1\n",
    "        AVG_REW.append(avg_rew)\n",
    "        MIN_REW.append(min_rew)\n",
    "        MAX_REW.append(max_rew)\n",
    "        rewards=np.array([0], dtype=np.float64)\n",
    "        avg_rew=np.array([0], dtype=np.float64)\n",
    "        min_rew=np.array([0], dtype=np.float64)\n",
    "        max_rew=np.array([0], dtype=np.float64)\n",
    "        \n",
    "        agent.model.save(f\"checkpoints/dqn_episode_{episode}.keras\")'''\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65114c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
